[2025-11-17 16:09:29.673] [7238] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 16:09:29.686] [7238] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 16:09:29.687] [7238] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 16:09:29.687] [7238] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 16:09:29.702] [7238] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 16:09:29.702] [7238] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 16:09:29.715] [7238] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 16:09:29.715] [7238] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 16:09:29.752] [7238] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 49.152239 milliseconds
[2025-11-17 16:09:29.753] [7238] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 50.659555 milliseconds
[2025-11-17 16:09:29.753] [7238] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 16:09:29.754] [7238] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 16:09:29.755] [7238] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 16:09:29.755] [7238] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 16:09:29.756] [7238] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 16:09:29.757] [7238] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 16:09:29.769] [7238] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 16:09:29.769] [7238] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 16:09:29.769] [7238] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 7245)
[2025-11-17 16:09:29.769] [7238] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 16:09:29.769] [7238] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 7246)
[2025-11-17 16:09:29.769] [7238] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 16:09:29.769] [7238] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] MultiPushQEl0YOLOX-Post-Process | inputs: AsyncHwEl[0] AsyncHwEl[1] AsyncHwEl[2] AsyncHwEl[3] AsyncHwEl[4] AsyncHwEl[5] AsyncHwEl[6] AsyncHwEl[7] AsyncHwEl[8] | outputs: NmsPPMuxEl0YOLOX-Post-Process(running in thread_id: 7247)
[2025-11-17 16:09:29.769] [7238] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] NmsPPMuxEl0YOLOX-Post-Process | inputs: MultiPushQEl0YOLOX-Post-Process[0] | outputs: LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process
[2025-11-17 16:09:29.769] [7238] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process | inputs: NmsPPMuxEl0YOLOX-Post-Process[0] | outputs: user
[2025-11-17 16:09:29.949] [7238] [HailoRT] [error] [infer_model.cpp:935] [validate_bindings] CHECK failed - Couldnt find output buffer for 'yolov6n/yolox_nms_postprocess'
[2025-11-17 16:09:29.949] [7238] [HailoRT] [error] [infer_model.cpp:945] [run_async] CHECK_SUCCESS failed with status=HAILO_NOT_FOUND(61)
[2025-11-17 16:09:29.949] [7238] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 16:09:29.951] [7238] [HailoRT] [error] [infer_model.cpp:651] [run_async] CHECK_SUCCESS failed with status=HAILO_NOT_FOUND(61)
[2025-11-17 16:18:54.185] [7836] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 16:18:54.198] [7836] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 16:18:54.199] [7836] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 16:18:54.199] [7836] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 16:18:54.214] [7836] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 16:18:54.214] [7836] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 16:18:54.228] [7836] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 16:18:54.228] [7836] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 16:18:54.265] [7836] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 50.13583 milliseconds
[2025-11-17 16:18:54.266] [7836] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 51.565565 milliseconds
[2025-11-17 16:18:54.266] [7836] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 16:18:54.267] [7836] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 16:18:54.268] [7836] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 16:18:54.268] [7836] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 16:18:54.270] [7836] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 16:18:54.270] [7836] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 16:18:54.282] [7836] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 16:18:54.283] [7836] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 16:18:54.283] [7836] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 7843)
[2025-11-17 16:18:54.283] [7836] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 16:18:54.283] [7836] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 7844)
[2025-11-17 16:18:54.283] [7836] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 16:18:54.283] [7836] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] MultiPushQEl0YOLOX-Post-Proce[2025-11-17 16:22:15.751] [8057] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 16:22:15.766] [8057] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 16:22:15.767] [8057] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 16:22:15.767] [8057] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 16:22:15.783] [8057] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 16:22:15.783] [8057] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 16:22:15.798] [8057] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 16:22:15.798] [8057] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 16:22:15.835] [8057] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 51.127503 milliseconds
[2025-11-17 16:22:15.836] [8057] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 52.564822 milliseconds
[2025-11-17 16:22:15.836] [8057] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 16:22:15.836] [8057] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 16:22:15.837] [8057] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 16:22:15.837] [8057] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 16:22:15.839] [8057] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 16:22:15.839] [8057] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 16:22:15.852] [8057] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 16:22:15.852] [8057] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 16:22:15.852] [8057] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 8064)
[2025-11-17 16:22:15.852] [8057] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 16:22:15.852] [8057] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 8065)
[2025-11-17 16:22:15.852] [8057] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 16:22:15.853] [8057] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] MultiPushQEl0YOLOX-Post-Proc[2025-11-17 16:25:16.385] [8351] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 16:25:16.400] [8351] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 16:25:16.400] [8351] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 16:25:16.401] [8351] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 16:25:16.416] [8351] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 16:25:16.417] [8351] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 16:25:16.428] [8351] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 16:25:16.428] [8351] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 16:25:16.466] [8351] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 48.224582 milliseconds
[2025-11-17 16:25:16.467] [8351] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 49.678527 milliseconds
[2025-11-17 16:25:16.467] [8351] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 16:25:16.467] [8351] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 16:25:16.468] [8351] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 16:25:16.468] [8351] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 16:25:16.470] [8351] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 16:25:16.470] [8351] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 16:25:16.482] [8351] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 16:25:16.482] [8351] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 16:25:16.482] [8351] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 8358)
[2025-11-17 16:25:16.482] [8351] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 16:25:16.482] [8351] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 8359)
[2025-11-17 16:25:16.482] [8351] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 16:25:16.483] [8351] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] MultiPushQEl0YOLOX-Post-Proc[2025-11-17 16:52:06.243] [8834] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 16:52:06.257] [8834] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 16:52:06.258] [8834] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 16:52:06.258] [8834] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 16:52:06.274] [8834] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 16:52:06.274] [8834] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 16:52:06.287] [8834] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 16:52:06.287] [8834] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 16:52:06.323] [8834] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 48.152121 milliseconds
[2025-11-17 16:52:06.324] [8834] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 49.605478 milliseconds
[2025-11-17 16:52:06.324] [8834] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 16:52:06.325] [8834] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 16:52:06.326] [8834] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 16:52:06.326] [8834] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 16:52:06.328] [8834] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 16:52:06.328] [8834] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 16:52:06.340] [8834] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 16:52:06.340] [8834] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 16:52:06.340] [8834] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 8841)
[2025-11-17 16:52:06.340] [8834] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 16:52:06.340] [8834] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 8842)
[2025-11-17 16:52:06.340] [8834] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 16:52:06.340] [8834] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] MultiPushQEl0YOLOX-Post-Proc[2025-11-17 17:10:02.386] [9527] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 17:10:02.400] [9527] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 17:10:02.401] [9527] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 17:10:02.401] [9527] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 17:10:02.417] [9527] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:10:02.417] [9527] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:10:02.430] [9527] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 17:10:02.430] [9527] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 17:10:02.466] [9527] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 48.291826 milliseconds
[2025-11-17 17:10:02.467] [9527] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 49.719518 milliseconds
[2025-11-17 17:10:02.467] [9527] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 17:10:02.468] [9527] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 17:10:02.469] [9527] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:10:02.469] [9527] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 17:10:02.470] [9527] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:10:02.471] [9527] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 17:10:02.483] [9527] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 17:10:02.483] [9527] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 17:10:02.483] [9527] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 9534)
[2025-11-17 17:10:02.483] [9527] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 17:10:02.483] [9527] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 9535)
[2025-11-17 17:10:02.483] [9527] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 17:10:02.483] [9527] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] MultiPushQEl0YOLOX-Post-Proc[2025-11-17 17:12:10.435] [9706] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 17:12:10.449] [9706] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 17:12:10.450] [9706] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 17:12:10.450] [9706] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 17:12:10.466] [9706] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:12:10.466] [9706] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:12:10.479] [9706] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 17:12:10.479] [9706] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 17:12:10.515] [9706] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 48.429206 milliseconds
[2025-11-17 17:12:10.516] [9706] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 49.85573 milliseconds
[2025-11-17 17:12:10.516] [9706] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 17:12:10.517] [9706] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 17:12:10.518] [9706] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:12:10.518] [9706] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 17:12:10.520] [9706] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:12:10.520] [9706] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 17:12:10.532] [9706] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 17:12:10.532] [9706] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 17:12:10.532] [9706] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 9713)
[2025-11-17 17:12:10.532] [9706] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 17:12:10.532] [9706] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 9714)
[2025-11-17 17:12:10.532] [9706] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 17:12:10.533] [9706] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] MultiPushQEl0YOLOX-Post-Process | inputs: AsyncHwEl[0] AsyncHwEl[1] AsyncHwEl[2] AsyncHwEl[3] AsyncHwEl[4] AsyncHwEl[5] AsyncHwEl[6] AsyncHwEl[7] AsyncHwEl[8] | outputs: NmsPPMuxEl0YOLOX-Post-Process(running in thread_id: 9715)
[2025-11-17 17:12:10.533] [9706] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] NmsPPMuxEl0YOLOX-Post-Process | inputs: MultiPushQEl0YOLOX-Post-Process[0] | outputs: LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process
[2025-11-17 17:12:10.533] [9706] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process | inputs: NmsPPMuxEl0YOLOX-Post-Process[0] | outputs: user
[2025-11-17 17:12:10.675] [9713] [HailoRT] [error] [dma_buffer_utils.cpp:37] [mmap_dma_buffer] CHECK failed - Failed to run mmap on DMA buffer
[2025-11-17 17:12:10.676] [9713] [HailoRT] [error] [pipeline.cpp:298] [set_dma_buf_as_memview] CHECK_SUCCESS failed with status=HAILO_INTERNAL_FAILURE(8)
[2025-11-17 17:12:10.676] [9713] [HailoRT] [error] [pipeline.cpp:186] [as_view] CHECK_SUCCESS failed with status=HAILO_INTERNAL_FAILURE(8)
[2025-11-17 17:12:10.676] [9713] [HailoRT] [error] [filter_elements.cpp:173] [action] CHECK_SUCCESS failed with status=HAILO_INTERNAL_FAILURE(8)
[2025-11-17 17:12:10.677] [9715] [HailoRT] [error] [pipeline_internal.cpp:26] [handle_non_recoverable_async_error] Non-recoverable Async Infer Pipeline error. status error code: HAILO_INTERNAL_FAILURE(8)
[2025-11-17 17:12:10.677] [9715] [HailoRT] [error] [async_infer_runner.cpp:88] [shutdown] Shutting down the pipeline with status HAILO_INTERNAL_FAILURE(8)
[2025-11-17 17:12:10.678] [9715] [HailoRT] [info] [queue_elements.cpp:1023] [operator()] Thread in element MultiPushQEl0YOLOX-Post-Process is not running anymore, exiting..
[2025-11-17 17:12:10.727] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_INTERNAL_FAILURE(8) - Can't handle inference request since pipeline status is HAILO_INTERNAL_FAILURE(8).
[2025-11-17 17:12:10.727] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_INTERNAL_FAILURE(8)
[2025-11-17 17:12:10.727] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.744] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.744] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.744] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.764] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.764] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.764] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.782] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.783] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.783] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.801] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.801] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.801] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.819] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.819] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.819] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.826] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.826] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.826] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.833] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.833] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.833] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.839] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.839] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.840] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.846] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.846] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.846] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.859] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.859] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.860] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.866] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.866] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.866] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.873] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.873] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.873] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.879] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.879] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.880] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.890] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.891] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.891] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.897] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.897] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.897] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.904] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.904] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.904] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.911] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.911] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.911] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.922] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.922] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.922] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.928] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.928] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.928] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.934] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.934] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.935] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.941] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.941] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.941] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.955] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.955] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.955] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.961] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.961] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.961] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.971] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.971] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.971] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.977] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.977] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.977] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.983] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.984] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.984] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:10.995] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:10.995] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:10.995] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.001] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.002] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.002] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.012] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.012] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.012] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.018] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.018] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.018] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.028] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.028] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.028] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.034] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.035] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.035] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.043] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.043] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.043] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.047] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.047] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.048] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.056] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.056] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.056] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.060] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.060] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.060] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.069] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.069] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.069] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.075] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.075] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.076] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.084] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.084] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.084] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.088] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.088] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.088] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.097] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.097] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.097] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.101] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.101] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.101] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.111] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.111] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.111] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.117] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.117] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.117] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.128] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.128] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.128] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.132] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.132] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.132] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.143] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.143] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.143] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.150] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.150] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.150] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.160] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.161] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.161] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.167] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.167] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.167] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.177] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.177] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.177] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.181] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.181] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.181] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.185] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.185] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.185] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.194] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.194] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.194] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.199] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.199] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.199] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.203] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.203] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.203] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.216] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.216] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.216] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.222] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.222] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.222] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.233] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.234] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.234] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.240] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.240] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.240] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.252] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.252] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.252] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.258] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.258] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.259] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.268] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.268] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.268] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.272] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.272] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.272] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.281] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.281] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.281] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.285] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.285] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.285] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.293] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.294] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.294] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.298] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.298] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.298] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.302] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.302] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.302] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.312] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.313] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.313] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.317] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.317] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.317] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.331] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.332] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.332] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.336] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.336] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.336] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.347] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.347] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.347] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.353] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.353] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.353] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.364] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.364] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.364] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.370] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.370] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.370] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.380] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.380] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.380] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.384] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.384] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.384] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.392] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.392] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.392] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.396] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.396] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.397] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.404] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.404] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.405] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.409] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.409] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.409] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.418] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.419] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.419] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.424] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.424] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.424] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.433] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.434] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.434] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.440] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.440] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.440] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.449] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.449] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.449] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.455] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.455] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.455] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.468] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.468] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.468] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.474] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.474] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.474] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.488] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.488] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.488] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.494] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.494] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.494] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.507] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.507] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.507] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.513] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.513] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.513] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.525] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.526] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.526] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.532] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.532] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.532] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.544] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.544] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.544] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.550] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.550] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.550] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.563] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.563] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.563] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.569] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.569] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.569] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.582] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.582] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.582] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.588] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.588] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.588] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.600] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.600] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.600] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.604] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.604] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.604] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.638] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.639] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.639] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.643] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.643] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.643] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.653] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.654] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.654] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.658] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.658] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.658] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.669] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.669] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.669] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.674] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.674] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.674] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.686] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.686] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.686] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.692] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.692] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.692] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.704] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.704] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.704] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.710] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.710] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.710] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.723] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.723] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.723] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.742] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.742] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.742] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.748] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.748] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.748] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.754] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.754] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.755] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.761] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.761] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.761] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.772] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.772] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.772] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.778] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.779] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.779] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.784] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.785] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.785] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.794] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.794] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.794] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.799] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.799] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.799] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.803] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.803] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.803] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.814] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.814] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.814] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.820] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.820] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.820] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.826] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.827] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.827] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.836] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.836] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.836] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.842] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.842] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.842] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.852] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.852] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.853] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.859] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.859] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.859] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.873] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.873] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.873] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.879] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.880] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.880] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.889] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.889] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.889] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.893] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.893] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.893] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.897] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.898] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.898] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.908] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.908] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.908] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.912] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.912] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.912] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.916] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.916] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.916] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.925] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.925] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.925] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.929] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.929] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.929] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.933] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.933] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.933] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.942] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.942] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.943] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.947] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.947] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.947] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.951] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.951] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.951] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.964] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.964] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.965] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.969] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.969] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.969] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.977] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.977] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.977] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.981] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.981] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.981] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.993] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.993] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.993] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:11.997] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:11.997] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:11.997] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.006] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.006] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.006] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.012] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.012] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.012] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.022] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.023] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.023] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.028] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.029] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.029] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.039] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.039] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.039] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.045] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.045] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.045] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.053] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.054] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.054] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.057] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.058] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.058] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.062] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.062] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.062] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.071] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.071] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.071] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.075] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.075] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.076] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.080] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.080] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.080] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.088] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.088] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.088] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.092] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.093] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.093] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.101] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.101] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.101] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.105] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.105] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.105] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.115] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.116] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.116] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.120] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.120] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.120] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.128] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.128] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.128] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.132] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.132] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.132] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.141] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.141] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.141] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.145] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.145] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.145] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.155] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.155] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.155] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.164] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.164] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:12:12.164] [9718] [HailoRT] [info] [async_infer_runner.cpp:86] [shutdown] Pipeline was aborted. Shutting it down
[2025-11-17 17:12:12.168] [9718] [HailoRT] [error] [async_infer_runner.cpp:286] [run] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63) - Can't handle inference request since pipeline status is HAILO_STREAM_ABORT(63).
[2025-11-17 17:12:12.168] [9718] [HailoRT] [error] [infer_model.cpp:970] [run_async] CHECK_SUCCESS failed with status=HAILO_STREAM_ABORT(63)
[2025-11-17 17:15:12.635] [9883] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 17:15:12.649] [9883] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 17:15:12.650] [9883] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 17:15:12.650] [9883] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 17:15:12.666] [9883] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:15:12.666] [9883] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:15:12.677] [9883] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 17:15:12.677] [9883] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 17:15:12.711] [9883] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 43.763185 milliseconds
[2025-11-17 17:15:12.712] [9883] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 45.195834 milliseconds
[2025-11-17 17:15:12.712] [9883] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 17:15:12.712] [9883] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 17:15:12.713] [9883] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:15:12.713] [9883] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 17:15:12.715] [9883] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:15:12.715] [9883] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 17:15:12.728] [9883] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 17:15:12.728] [9883] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 17:15:12.728] [9883] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 9891)
[2025-11-17 17:15:12.728] [9883] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 17:15:12.728] [9883] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 9892)
[2025-11-17 17:15:12.728] [9883] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 17:15:12.728] [9883] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] MultiPushQEl0YOLOX-Post-Proc[2025-11-17 17:21:32.486] [10207] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 17:21:32.501] [10207] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 17:21:32.502] [10207] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 17:21:32.502] [10207] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 17:21:32.517] [10207] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:21:32.517] [10207] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:21:32.532] [10207] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 17:21:32.532] [10207] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 17:21:32.567] [10207] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 48.825299 milliseconds
[2025-11-17 17:21:32.568] [10207] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 50.181533 milliseconds
[2025-11-17 17:21:32.568] [10207] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 17:21:32.569] [10207] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 17:21:32.569] [10207] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:21:32.570] [10207] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 17:21:32.571] [10207] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:21:32.572] [10207] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 17:21:32.584] [10207] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 17:21:32.584] [10207] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 17:21:32.584] [10207] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 10214)
[2025-11-17 17:21:32.584] [10207] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 17:21:32.584] [10207] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 10215)
[2025-11-17 17:21:32.584] [10207] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 17:21:32.584] [10207] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] Mul[2025-11-17 17:21:50.807] [10341] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 17:21:50.821] [10341] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 17:21:50.822] [10341] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 17:21:50.822] [10341] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 17:21:50.838] [10341] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:21:50.838] [10341] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:21:50.851] [10341] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 17:21:50.851] [10341] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 17:21:50.885] [10341] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 46.534868 milliseconds
[2025-11-17 17:21:50.886] [10341] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 47.971601 milliseconds
[2025-11-17 17:21:50.887] [10341] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 17:21:50.887] [10341] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 17:21:50.888] [10341] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:21:50.888] [10341] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 17:21:50.890] [10341] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:21:50.890] [10341] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 17:21:50.902] [10341] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 17:21:50.902] [10341] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 17:21:50.902] [10341] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 10348)
[2025-11-17 17:21:50.902] [10341] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 17:21:50.902] [10341] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 10349)
[2025-11-17 17:21:50.902] [10341] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 17:21:50.902] [10341] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] Mul[2025-11-17 17:30:45.870] [10712] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 17:30:45.884] [10712] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 17:30:45.885] [10712] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 17:30:45.885] [10712] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 17:30:45.901] [10712] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:30:45.901] [10712] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:30:45.912] [10712] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 17:30:45.912] [10712] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 17:30:45.950] [10712] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 48.113645 milliseconds
[2025-11-17 17:30:45.951] [10712] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 49.518587 milliseconds
[2025-11-17 17:30:45.951] [10712] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 17:30:45.952] [10712] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 17:30:45.952] [10712] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:30:45.953] [10712] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 17:30:45.955] [10712] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:30:45.955] [10712] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 17:30:45.967] [10712] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 17:30:45.967] [10712] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 17:30:45.967] [10712] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 10719)
[2025-11-17 17:30:45.967] [10712] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 17:30:45.967] [10712] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 10720)
[2025-11-17 17:30:45.968] [10712] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 17:30:45.968] [10712] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] Mul[2025-11-17 17:31:38.107] [10785] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 17:31:38.121] [10785] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 17:31:38.122] [10785] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 17:31:38.122] [10785] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 17:31:38.138] [10785] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:31:38.138] [10785] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:31:38.149] [10785] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 17:31:38.149] [10785] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 17:31:38.185] [10785] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 46.738161 milliseconds
[2025-11-17 17:31:38.186] [10785] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 48.167894 milliseconds
[2025-11-17 17:31:38.187] [10785] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 17:31:38.187] [10785] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 17:31:38.188] [10785] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:31:38.188] [10785] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 17:31:38.190] [10785] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:31:38.190] [10785] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 17:31:38.202] [10785] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 17:31:38.202] [10785] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 17:31:38.202] [10785] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 10792)
[2025-11-17 17:31:38.202] [10785] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 17:31:38.202] [10785] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 10793)
[2025-11-17 17:31:38.202] [10785] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 17:31:38.202] [10785] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] Mul[2025-11-17 17:31:46.380] [10842] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 17:31:46.394] [10842] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 17:31:46.395] [10842] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 17:31:46.395] [10842] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 17:31:46.410] [10842] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:31:46.410] [10842] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:31:46.425] [10842] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 17:31:46.425] [10842] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 17:31:46.462] [10842] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 51.202359 milliseconds
[2025-11-17 17:31:46.463] [10842] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 52.648425 milliseconds
[2025-11-17 17:31:46.464] [10842] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 17:31:46.464] [10842] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 17:31:46.465] [10842] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:31:46.465] [10842] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 17:31:46.467] [10842] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:31:46.467] [10842] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 17:31:46.479] [10842] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 17:31:46.479] [10842] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 17:31:46.479] [10842] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 10849)
[2025-11-17 17:31:46.479] [10842] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 17:31:46.479] [10842] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 10850)
[2025-11-17 17:31:46.479] [10842] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 17:31:46.479] [10842] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] Mul[2025-11-17 17:31:56.727] [10903] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 17:31:56.742] [10903] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 17:31:56.743] [10903] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 17:31:56.743] [10903] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 17:31:56.759] [10903] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:31:56.759] [10903] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:31:56.771] [10903] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 17:31:56.771] [10903] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 17:31:56.808] [10903] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 48.185103 milliseconds
[2025-11-17 17:31:56.809] [10903] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 49.64896 milliseconds
[2025-11-17 17:31:56.809] [10903] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 17:31:56.809] [10903] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 17:31:56.810] [10903] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:31:56.810] [10903] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 17:31:56.812] [10903] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:31:56.812] [10903] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 17:31:56.824] [10903] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 17:31:56.824] [10903] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 17:31:56.824] [10903] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 10910)
[2025-11-17 17:31:56.824] [10903] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 17:31:56.824] [10903] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 10911)
[2025-11-17 17:31:56.825] [10903] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 17:31:56.825] [10903] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] Mult[2025-11-17 17:32:01.547] [10946] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 17:32:01.562] [10946] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 17:32:01.563] [10946] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 17:32:01.563] [10946] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 17:32:01.578] [10946] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:32:01.578] [10946] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:32:01.592] [10946] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 17:32:01.592] [10946] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 17:32:01.630] [10946] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 50.805113 milliseconds
[2025-11-17 17:32:01.631] [10946] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 52.229305 milliseconds
[2025-11-17 17:32:01.631] [10946] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 17:32:01.632] [10946] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 17:32:01.632] [10946] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:32:01.633] [10946] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 17:32:01.634] [10946] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:32:01.635] [10946] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 17:32:01.647] [10946] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 17:32:01.647] [10946] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 17:32:01.647] [10946] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 10953)
[2025-11-17 17:32:01.647] [10946] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 17:32:01.647] [10946] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 10954)
[2025-11-17 17:32:01.647] [10946] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 17:32:01.647] [10946] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] Mul[2025-11-17 17:32:23.035] [11005] [HailoRT] [info] [vdevice.cpp:535] [create] Creating vdevice with params: device_count: 1, scheduling_algorithm: ROUND_ROBIN, multi_process_service: false
[2025-11-17 17:32:23.049] [11005] [HailoRT] [info] [device.cpp:51] [Device] OS Version: Linux 6.1.84+ #1 SMP Mon Sep 29 06:59:38 MSK 2025 aarch64
[2025-11-17 17:32:23.050] [11005] [HailoRT] [info] [control.cpp:113] [control__parse_identify_results] firmware_version is: 4.23.0
[2025-11-17 17:32:23.050] [11005] [HailoRT] [info] [vdevice.cpp:682] [create] VDevice Infos: 0000:01:00.0
[2025-11-17 17:32:23.066] [11005] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:32:23.066] [11005] [HailoRT] [info] [hef.cpp:1994] [get_network_group_and_network_name] No name was given. Addressing all networks of default network_group: yolov6n
[2025-11-17 17:32:23.080] [11005] [HailoRT] [info] [internal_buffer_manager.cpp:75] [print_execution_results] Default Internal buffer planner failed to meet requirements
[2025-11-17 17:32:23.080] [11005] [HailoRT] [info] [internal_buffer_manager.cpp:86] [print_execution_results] Default Internal buffer planner executed successfully
[2025-11-17 17:32:23.115] [11005] [HailoRT] [info] [device_internal.cpp:57] [configure] Configuring HEF took 48.164686 milliseconds
[2025-11-17 17:32:23.116] [11005] [HailoRT] [info] [vdevice.cpp:780] [configure] Configuring HEF on VDevice took 49.585669 milliseconds
[2025-11-17 17:32:23.116] [11005] [HailoRT] [info] [infer_model.cpp:417] [configure] Configuring network group 'yolov6n' with params: batch size: 0, power mode: PERFORMANCE, latency: NONE
[2025-11-17 17:32:23.117] [11005] [HailoRT] [info] [multi_io_elements.cpp:754] [create] Created (AsyncHwEl)
[2025-11-17 17:32:23.118] [11005] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (EntryPushQEl0yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:32:23.118] [11005] [HailoRT] [info] [filter_elements.cpp:101] [create] Created (PreInferEl3yolov6n/input_layer1 | Reorder - src_order: NHWC, src_shape: (640, 640, 3), dst_order: NHCW, dst_shape: (640, 640, 3))
[2025-11-17 17:32:23.120] [11005] [HailoRT] [info] [queue_elements.cpp:450] [create] Created (PushQEl3yolov6n/input_layer1 | timeout: 10s)
[2025-11-17 17:32:23.120] [11005] [HailoRT] [info] [multi_io_elements.cpp:135] [create] Created (NmsPPMuxEl0YOLOX-Post-Process | Op YOLOX, Name: YOLOX-Post-Process, Score threshold: 0.200, IoU threshold: 0.65, Classes: 80, Max bboxes per class: 100, Image height: 640, Image width: 640)
[2025-11-17 17:32:23.132] [11005] [HailoRT] [info] [queue_elements.cpp:942] [create] Created (MultiPushQEl0YOLOX-Post-Process | timeout: 10s)
[2025-11-17 17:32:23.132] [11005] [HailoRT] [info] [edge_elements.cpp:187] [create] Created (LastAsyncEl0NmsPPMuxEl0YOLOX-Post-Process)
[2025-11-17 17:32:23.132] [11005] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] EntryPushQEl0yolov6n/input_layer1 | inputs: user | outputs: PreInferEl3yolov6n/input_layer1(running in thread_id: 11012)
[2025-11-17 17:32:23.132] [11005] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PreInferEl3yolov6n/input_layer1 | inputs: EntryPushQEl0yolov6n/input_layer1[0] | outputs: PushQEl3yolov6n/input_layer1
[2025-11-17 17:32:23.132] [11005] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] PushQEl3yolov6n/input_layer1 | inputs: PreInferEl3yolov6n/input_layer1[0] | outputs: AsyncHwEl(running in thread_id: 11013)
[2025-11-17 17:32:23.132] [11005] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] AsyncHwEl | inputs: PushQEl3yolov6n/input_layer1[0] | outputs: MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process MultiPushQEl0YOLOX-Post-Process
[2025-11-17 17:32:23.132] [11005] [HailoRT] [info] [pipeline.cpp:891] [print_deep_description] Mul